{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis - `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/line/anaconda3/envs/mvpa/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to load fast implementation of SMLR.  May be you forgotten to build it.  We will use much slower pure-Python version. Original exception was no file with expected extension\n",
      " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
      "WARNING: SMLR: C implementation is not available. Using pure Python one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/line/anaconda3/envs/mvpa/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from mvpa2.tutorial_suite import fmri_dataset\n",
    "from mvpa2.base.hdf5 import h5save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filename, outlier_thr=5):\n",
    "\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # extract relevant variables\n",
    "    sub_id = df['PAC_ID']\n",
    "    df = df.drop('PAC_ID', 1)\n",
    "    header = df.keys()\n",
    "    \n",
    "    # Clean dataset - drop subjects with values above `outlier_thr` STD\n",
    "    outliers = np.sum((np.abs(zscore(df)) > outlier_thr), 1) != 0\n",
    "    print('%d outliers detected.' % outliers.sum())\n",
    "    data = np.array(df.drop(np.where(outliers)[0]))\n",
    "    sub_id = sub_id[np.invert(outliers)]\n",
    "    \n",
    "    # zscore data\n",
    "    data = zscore(data)\n",
    "\n",
    "    # Reset Gender and Scanner values to nominal values\n",
    "    data[:,0] = (data[:,0]>0) + 1\n",
    "    data[:,2] = (data[:,2]>0) + 1\n",
    "    data[:,4] = [np.where(i==np.unique(data[:,4]))[0][0] + 1for i in data[:,4]]\n",
    "\n",
    "    return pd.DataFrame(data, columns=header), sub_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 outliers detected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>TIV</th>\n",
       "      <th>Scanner</th>\n",
       "      <th>Tvoxels</th>\n",
       "      <th>Tmean</th>\n",
       "      <th>Tmedian</th>\n",
       "      <th>Tstd</th>\n",
       "      <th>Tmax</th>\n",
       "      <th>...</th>\n",
       "      <th>Right_Cerebral_White_Matter</th>\n",
       "      <th>Right_Cerebral_Cortex</th>\n",
       "      <th>Right_Lateral_Ventricle</th>\n",
       "      <th>Right_Thalamus</th>\n",
       "      <th>Right_Caudate</th>\n",
       "      <th>Right_Putamen</th>\n",
       "      <th>Right_Pallidum</th>\n",
       "      <th>Right_Hippocampus</th>\n",
       "      <th>Right_Amygdala</th>\n",
       "      <th>Right_Accumbens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.610405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.479924</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.890455</td>\n",
       "      <td>1.091014</td>\n",
       "      <td>1.080367</td>\n",
       "      <td>1.034026</td>\n",
       "      <td>0.060878</td>\n",
       "      <td>...</td>\n",
       "      <td>1.093750</td>\n",
       "      <td>1.094903</td>\n",
       "      <td>1.037705</td>\n",
       "      <td>-0.896948</td>\n",
       "      <td>-0.255150</td>\n",
       "      <td>-0.724690</td>\n",
       "      <td>-0.269052</td>\n",
       "      <td>1.791264</td>\n",
       "      <td>1.591691</td>\n",
       "      <td>0.091529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.146076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.052883</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.134709</td>\n",
       "      <td>-0.118805</td>\n",
       "      <td>-0.147065</td>\n",
       "      <td>0.290905</td>\n",
       "      <td>0.904814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060199</td>\n",
       "      <td>-0.013714</td>\n",
       "      <td>0.079673</td>\n",
       "      <td>0.634875</td>\n",
       "      <td>1.097503</td>\n",
       "      <td>0.176912</td>\n",
       "      <td>-0.143525</td>\n",
       "      <td>0.223282</td>\n",
       "      <td>0.283818</td>\n",
       "      <td>-0.110870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.200997</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.322187</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.910314</td>\n",
       "      <td>0.213228</td>\n",
       "      <td>0.466651</td>\n",
       "      <td>-0.390523</td>\n",
       "      <td>0.488056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167886</td>\n",
       "      <td>0.162364</td>\n",
       "      <td>0.443724</td>\n",
       "      <td>0.107536</td>\n",
       "      <td>-0.337023</td>\n",
       "      <td>-0.458773</td>\n",
       "      <td>-0.199329</td>\n",
       "      <td>0.879846</td>\n",
       "      <td>0.603997</td>\n",
       "      <td>0.182364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.200997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.526994</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.888128</td>\n",
       "      <td>1.178024</td>\n",
       "      <td>1.141739</td>\n",
       "      <td>1.173458</td>\n",
       "      <td>0.863138</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022908</td>\n",
       "      <td>1.000870</td>\n",
       "      <td>2.157792</td>\n",
       "      <td>1.573471</td>\n",
       "      <td>2.835140</td>\n",
       "      <td>1.080137</td>\n",
       "      <td>0.815276</td>\n",
       "      <td>0.185643</td>\n",
       "      <td>0.730864</td>\n",
       "      <td>2.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.004188</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.934370</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.027570</td>\n",
       "      <td>-2.074238</td>\n",
       "      <td>-2.049584</td>\n",
       "      <td>-2.070608</td>\n",
       "      <td>-0.824733</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.148225</td>\n",
       "      <td>-2.165083</td>\n",
       "      <td>-1.193547</td>\n",
       "      <td>-1.593558</td>\n",
       "      <td>-2.270355</td>\n",
       "      <td>-1.290505</td>\n",
       "      <td>-1.337832</td>\n",
       "      <td>-1.919742</td>\n",
       "      <td>-2.127827</td>\n",
       "      <td>-1.856324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label       Age  Gender       TIV  Scanner   Tvoxels     Tmean   Tmedian  \\\n",
       "0    1.0  1.610405     1.0  1.479924      2.0 -0.890455  1.091014  1.080367   \n",
       "1    1.0 -1.146076     1.0 -0.052883      1.0  1.134709 -0.118805 -0.147065   \n",
       "2    1.0 -0.200997     2.0 -0.322187      2.0 -0.910314  0.213228  0.466651   \n",
       "3    1.0 -0.200997     1.0  1.526994      3.0 -0.888128  1.178024  1.141739   \n",
       "4    1.0  2.004188     2.0 -0.934370      1.0  1.027570 -2.074238 -2.049584   \n",
       "\n",
       "       Tstd      Tmax       ...         Right_Cerebral_White_Matter  \\\n",
       "0  1.034026  0.060878       ...                            1.093750   \n",
       "1  0.290905  0.904814       ...                           -0.060199   \n",
       "2 -0.390523  0.488056       ...                            0.167886   \n",
       "3  1.173458  0.863138       ...                            1.022908   \n",
       "4 -2.070608 -0.824733       ...                           -2.148225   \n",
       "\n",
       "   Right_Cerebral_Cortex  Right_Lateral_Ventricle  Right_Thalamus  \\\n",
       "0               1.094903                 1.037705       -0.896948   \n",
       "1              -0.013714                 0.079673        0.634875   \n",
       "2               0.162364                 0.443724        0.107536   \n",
       "3               1.000870                 2.157792        1.573471   \n",
       "4              -2.165083                -1.193547       -1.593558   \n",
       "\n",
       "   Right_Caudate  Right_Putamen  Right_Pallidum  Right_Hippocampus  \\\n",
       "0      -0.255150      -0.724690       -0.269052           1.791264   \n",
       "1       1.097503       0.176912       -0.143525           0.223282   \n",
       "2      -0.337023      -0.458773       -0.199329           0.879846   \n",
       "3       2.835140       1.080137        0.815276           0.185643   \n",
       "4      -2.270355      -1.290505       -1.337832          -1.919742   \n",
       "\n",
       "   Right_Amygdala  Right_Accumbens  \n",
       "0        1.591691         0.091529  \n",
       "1        0.283818        -0.110870  \n",
       "2        0.603997         0.182364  \n",
       "3        0.730864         2.010200  \n",
       "4       -2.127827        -1.856324  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, sub_id = read_dataset('data/PAC2018_Covariates_detailed.csv', outlier_thr=5)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for PyMVPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance datasets (1 for `Scanner 1` and 1 for `Scanner 2/3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scan1_id = data['Scanner'] == 1\n",
    "labels1 = np.array(data[scan1_id]['Label'])\n",
    "sub_id1 = np.array(sub_id)[scan1_id]\n",
    "\n",
    "scan23_id = data['Scanner'] != 1\n",
    "labels23 = np.array(data[scan23_id]['Label'])\n",
    "sub_id23 = np.array(sub_id)[scan23_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_dataset(sub_id, labels):\n",
    "    max_label_size = np.min([np.sum(lab == labels) \n",
    "                             for lab in np.unique(labels)])\n",
    "\n",
    "    labels_1 = np.where(labels == 1)[0]\n",
    "    np.random.shuffle(labels_1)\n",
    "    labels_1 = labels_1[:max_label_size]\n",
    "\n",
    "    labels_2 = np.where(labels == 2)[0]\n",
    "    np.random.shuffle(labels_2)\n",
    "    labels_2 = labels_2[:max_label_size]\n",
    "\n",
    "    new_data_id = np.hstack((labels_1, labels_2))\n",
    "    labels = labels[new_data_id]\n",
    "    sub_id = sub_id[new_data_id]\n",
    "\n",
    "    return (sub_id, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1,  label1  = balance_dataset(sub_id1, labels1)\n",
    "sub23, label23 = balance_dataset(sub_id23, labels23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n"
     ]
    }
   ],
   "source": [
    "data = ['data/nifti/%s.nii.gz' % s for s in sub1]\n",
    "ds = fmri_dataset(samples=data,\n",
    "                  targets=label1,\n",
    "                  chunks=np.zeros(label1.shape),\n",
    "                  mask='data/mask_gm.nii.gz')\n",
    "del ds.sa['time_coords']\n",
    "del ds.sa['time_indices']\n",
    "\n",
    "h5save('data/mvpa_dataset_scanner1.hdf5', ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['data/nifti/%s.nii.gz' % s for s in sub23]\n",
    "ds = fmri_dataset(samples=data,\n",
    "                  targets=label23,\n",
    "                  chunks=np.zeros(label23.shape),\n",
    "                  mask='data/mask_gm.nii.gz')\n",
    "del ds.sa['time_coords']\n",
    "del ds.sa['time_indices']\n",
    "\n",
    "h5save('data/mvpa_dataset_scanner23.hdf5', ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchlight approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as opj\n",
    "from mvpa2.base.hdf5 import h5load, h5save\n",
    "#from mvpa2.clfs.svm import LinearNuSVM\n",
    "from mvpa2.clfs.gnb import GNB\n",
    "from mvpa2.clfs.smlr import SMLR\n",
    "from mvpa2.generators.partition import NFoldPartitioner\n",
    "from mvpa2.measures.base import CrossValidation\n",
    "from mvpa2.measures.searchlight import sphere_searchlight\n",
    "from mvpa2.misc.errorfx import mean_match_accuracy\n",
    "from mvpa2.mappers.fx import mean_sample\n",
    "from mvpa2.mappers.zscore import zscore\n",
    "from mvpa2.suite import map2nifti, time\n",
    "\n",
    "# Debugger on or off\n",
    "if __debug__:\n",
    "    from mvpa2.base import debug\n",
    "    debug.active = ['SLC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SLC] DBG:                        Starting off 8 child processes for nblocks=8\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:00:00 _______[0%]_______ -0:00:33  ROI 47601 (1/476), 33 features[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:00:00 _______[0%]_______ -0:00:44  ROI 101 (2/476), 20 featureses[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:00:00 _______[0%]_______ -0:00:48  ROI 142801 (1/476), 33 features[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:00:54 ======[99%]======_ -0:00:00  ROI 142401 (473/476), 33 features\n",
      "[SLC] DBG:                              +0:00:54 ======[100%]=====_ -0:00:00  ROI 380501 (474/476), 31 features\n",
      "[SLC] DBG:                              +0:00:54 ======[100%]====== 0:00:00  ROI 285501 (476/476), 33 features \n",
      "[SLC] DBG:                              +0:00:54 ======[100%]====== 0:00:00  ROI 380701 (476/476), 22 features \n",
      "[SLC] DBG:                              +0:00:54 ======[100%]=====_ -0:00:00  ROI 47401 (475/476), 20 features \n",
      "\n",
      "[SLC] DBG:                              +0:00:54 ======[100%]====== 0:00:00  ROI 237901 (476/476), 33 features \n",
      "[SLC] DBG:                              +0:00:54 ======[100%]====== 0:00:00  ROI 47501 (476/476), 33 features \n",
      "orig done after 00:01:01\n",
      "0.609990749306198\n",
      "[SLC] DBG:                        Starting off 8 child processes for nblocks=8\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:00:00 _______[0%]_______ -0:00:55  ROI 1 (1/476), 16 features[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:00:00 _______[0%]_______ -0:00:48  ROI 47601 (1/476), 33 features[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:00:00 _______[0%]_______ -0:00:58  ROI 142801 (1/476), 33 features[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              Starting computing block for 476 elements\n",
      "[SLC] DBG:                              +0:01:10 ======[99%]======_ -0:00:00  ROI 94701 (472/476), 33 features \n",
      "[SLC] DBG:                              +0:01:10 ======[98%]======_ -0:00:01  ROI 46701 (468/476), 33 features \n",
      "[SLC] DBG:                              +0:01:10 ======[100%]====== 0:00:00  ROI 95101 (476/476), 33 features s\n",
      "[SLC] DBG:                              +0:01:10 ======[100%]====== 0:00:00  ROI 142701 (476/476), 28 features \n",
      "[SLC] DBG:                              +0:01:10 ======[100%]====== 0:00:00  ROI 333101 (476/476), 17 features \n",
      "[SLC] DBG:                              +0:01:11 ======[100%]====== 0:00:00  ROI 190301 (476/476), 25 features \n",
      "[SLC] DBG:                              +0:01:11 ======[100%]====== 0:00:00  ROI 47501 (476/476), 33 features s\n",
      "[SLC] DBG:                              +0:01:11 ======[100%]====== 0:00:00  ROI 285501 (476/476), 33 features \n",
      "orig done after 00:01:19\n",
      "0.6015321756894789\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# 1. Parameters\n",
    "sphere_radius = 2\n",
    "nth_element = 100\n",
    "clfmode = 'SMLR'\n",
    "clfmode = 'LinearNuSVMC'\n",
    "clfmode = 'GNB'\n",
    "\n",
    "cores2use = 8\n",
    "\n",
    "for postfix in ['_scanner1', '_scanner23']:\n",
    "\n",
    "    ###\n",
    "    # 2. Create Dataset\n",
    "    ds = h5load('data/mvpa_dataset%s.hdf5' % postfix)\n",
    "\n",
    "    # Specify chunks for the cross validation - Note: This can be done here,\n",
    "    # because the subjects are well ordered (1/2 controls, 1/2 patients)\n",
    "    nchunks = 5\n",
    "    chunks = np.zeros(len(ds))\n",
    "\n",
    "    for i, e in enumerate(np.array_split(\n",
    "            range(int(len(ds) / 2.)), nchunks)):\n",
    "        chunks[e] = i\n",
    "\n",
    "    for i, e in enumerate(np.array_split(\n",
    "            range(int(len(ds) / 2.), len(ds)), nchunks)):\n",
    "        chunks[e] = i\n",
    "\n",
    "    ds.chunks = chunks\n",
    "\n",
    "    # Standardize your data\n",
    "    zscore(ds)\n",
    "\n",
    "    ###\n",
    "    # 3. Group Searchlight\n",
    "    outpath = './results/searchlight'\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "\n",
    "    def fill_in_scattered_results(sl, dataset, roi_ids, results):\n",
    "        \"\"\"Function to aggregate results - This requires the searchlight\n",
    "        conditional attribute 'roi_feature_ids' to be enabled\"\"\"\n",
    "        import numpy as np\n",
    "        from mvpa2.datasets import Dataset\n",
    "        resmap = None\n",
    "        for resblock in results:\n",
    "            for res in resblock:\n",
    "                if resmap is None:\n",
    "                    # prepare the result container\n",
    "                    resmap = np.zeros((len(res), dataset.nfeatures),\n",
    "                                      dtype=res.samples.dtype)\n",
    "                    observ_counter = np.zeros(dataset.nfeatures, dtype=int)\n",
    "                # project the result onto all features -- love broadcasting!\n",
    "                resmap[:, res.a.roi_feature_ids] += res.samples\n",
    "                # increment observation counter for all relevant features\n",
    "                observ_counter[res.a.roi_feature_ids] += 1\n",
    "        # when all results have been added up average them according to the number\n",
    "        # of observations\n",
    "        observ_mask = observ_counter > 0\n",
    "        resmap[:, observ_mask] /= observ_counter[observ_mask]\n",
    "        result_ds = Dataset(resmap,\n",
    "                            fa={'observations': observ_counter})\n",
    "        if 'mapper' in dataset.a:\n",
    "            import copy\n",
    "            result_ds.a['mapper'] = copy.copy(dataset.a.mapper)\n",
    "        return result_ds\n",
    "\n",
    "    # specify partitioner\n",
    "    partitioner = NFoldPartitioner(cvtype=1)\n",
    "\n",
    "    # Choose right classifier\n",
    "    if clfmode == 'GNB':\n",
    "        clf = GNB()\n",
    "\n",
    "    elif clfmode == 'LinearNuSVMC':\n",
    "        clf = LinearNuSVMC()\n",
    "\n",
    "    elif clfmode == 'SMLR':\n",
    "        smlr_lm = 0.1\n",
    "        clf = SMLR()\n",
    "\n",
    "\n",
    "    cv = CrossValidation(clf, partitioner,\n",
    "                         errorfx=mean_match_accuracy,\n",
    "                         enable_ca=['stats'])\n",
    "\n",
    "    sl = sphere_searchlight(cv,\n",
    "                            radius=sphere_radius,\n",
    "                            center_ids=range(0,\n",
    "                                             ds.shape[1],\n",
    "                                             nth_element),\n",
    "                            space='voxel_indices',\n",
    "                            results_fx=fill_in_scattered_results,\n",
    "                            postproc=mean_sample(),\n",
    "                            enable_ca=['calling_time', 'roi_feature_ids'])\n",
    "\n",
    "    # How many cores should be used\n",
    "    sl.nproc = cores2use\n",
    "\n",
    "    # Specify identifier\n",
    "    identifier = '%02dr_%03de_%s_%02dc%s' % (\n",
    "        sphere_radius, nth_element, clfmode, nchunks, postfix)\n",
    "\n",
    "    # train classifier on original and permutated dataset\n",
    "    ofname = opj(outpath, 'sl_clf_%s.hdf5' % identifier)\n",
    "    sl_map = sl(ds)\n",
    "    h5save(ofname, sl_map, compression=9)\n",
    "    print('orig done after %s' % (\n",
    "        time.strftime(\n",
    "            '%H:%M:%S',\n",
    "            time.gmtime(round(sl.ca.calling_time)))))\n",
    "\n",
    "    # Calculate Classifier Specific outputs\n",
    "    accuracies = sl_map.samples[0]\n",
    "    mean_accuracy = accuracies.mean()\n",
    "    std_accuracy = accuracies.std()\n",
    "    chance_level = 0.5\n",
    "\n",
    "    def threshold_above_average(x):\n",
    "        return chance_level + x * std_accuracy\n",
    "\n",
    "\n",
    "    def spheres_above_average(x):\n",
    "        return np.sum(accuracies >= threshold_above_average(x))\n",
    "\n",
    "\n",
    "    def percent_above_average(x):\n",
    "        return np.mean(accuracies >= threshold_above_average(x)) * 100\n",
    "\n",
    "\n",
    "    # Save searchlight accuracy map to NIfTI file\n",
    "    niftiresults = map2nifti(ds, data=sl_map.S, imghdr=ds.a.imghdr)\n",
    "    niftiresults.to_filename(opj(outpath, 'nifti_%s.nii.gz' % identifier))\n",
    "\n",
    "    # Write report to file\n",
    "    csvfile = opj(outpath, 'report_%s.rst' % identifier)\n",
    "    with open(csvfile, 'w') as f:\n",
    "        f.write('Classifier       : {0}\\n'.format(clfmode))\n",
    "        f.write('Postfix          : {0}\\n'.format(postfix))\n",
    "        f.write('Chunks           : {0}\\n'.format(nchunks))\n",
    "        f.write('Sphere Radius    : {0}\\n'.format(sphere_radius))\n",
    "        f.write('N-th Element     : {0}\\n'.format(nth_element))\n",
    "        f.write('Wall Time        : {0}\\n'.format(\n",
    "            time.strftime('%H:%M:%S', time.gmtime(round(sl.ca.calling_time)))))\n",
    "        f.write('Samples          : {0}\\n'.format(ds.S.shape[0]))\n",
    "        f.write('Features         : {0}\\n'.format(ds.S.shape[1]))\n",
    "        f.write('Volume Dimension : {0}\\n'.format(str(ds.a.voxel_dim)))\n",
    "        f.write('Voxel  Dimension : {0}\\n'.format(str(ds.a.voxel_eldim)))\n",
    "        f.write('CPU              : {0}\\n'.format(cores2use))\n",
    "\n",
    "        f.write('\\nChance Level     : {0}\\n'.format(round(chance_level, 5)))\n",
    "        f.write('Accuracy (mean)  : {0:5}\\n'.format(round(mean_accuracy, 5)))\n",
    "        f.write('Accuracy (std)   : {0:5}\\n\\n'.format(round(std_accuracy, 5)))\n",
    "\n",
    "        f.write(\n",
    "            '%Sphere > +2STD  : {0:5}%\\n'.format(round(\n",
    "                percent_above_average(2), 3)))\n",
    "        f.write(\n",
    "            '%Sphere > +3STD  : {0:5}%\\n'.format(round(\n",
    "                percent_above_average(3), 3)))\n",
    "        f.write(\n",
    "            '%Sphere > +4STD  : {0:5}%\\n'.format(round(\n",
    "                percent_above_average(4), 3)))\n",
    "        f.write('vSphere > +2STD  : {0:5}\\n'.format(spheres_above_average(2)))\n",
    "        f.write('vSphere > +3STD  : {0:5}\\n'.format(spheres_above_average(3)))\n",
    "        f.write('vSphere > +4STD  : {0:5}\\n'.format(spheres_above_average(4)))\n",
    "\n",
    "        f.write('\\n\\nDataset Summary:\\n')\n",
    "        f.write('****************\\n')\n",
    "        f.write('%s' % ds.summary())\n",
    "        f.write('%s' % ds.summary)\n",
    "\n",
    "    print(accuracies.max())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mvpa]",
   "language": "python",
   "name": "conda-env-mvpa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
